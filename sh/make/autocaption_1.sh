#!/bin/bash

# Configuration (Modify these!)
VIDEO_FOLDER="/path/to/your/video/folder"
BLENDER_EXECUTABLE="/usr/bin/blender"
OUTPUT_FOLDER="/path/to/output/folder"
OUTPUT_FILENAME="video_with_subtitles"

# Create output folder if it doesn't exist
mkdir -p "$OUTPUT_FOLDER"

# Function to process a single video file
process_video() {
  VIDEO_FILE="$1"
  BASE_NAME=$(basename "$VIDEO_FILE")
  VIDEO_NAME="${BASE_NAME%.*}"
  OUTPUT_VIDEO_FILE="$OUTPUT_FOLDER/${OUTPUT_FIOUTPUT_VIDEO_FILE="$OUTPUT_FOLDER/${OUTPUT_FILENAME}_${VIDEO_NAME}.mp4"

  echo "Processing: $VIDEO_FILE"

  # 1. Blender Audio-to-Text
  echo "Generating transcript with Blender..."

  # Blender command to generate transcript and
export video with subtitles.  Adapt this!
  "$BLENDER_EXECUTABLE" --background --python
/path/to/your/script.py --script-binding
"$VIDEO_FILE" --output "$OUTPUT_VIDEO_FILE"

  echo "Finished processing: $VIDEO_FILE"
}

# Find all video files in the folder
VIDEO_FILES=($(find "$VIDEO_FOLDER" -type f
-name "*.mp4" -o -name "*.avi" -o -name
"*.mov"))

# Loop through each video file
for VIDEO_FILE in "${VIDEO_FILES[@]}"; do
  process_video "$VIDEO_FILE"
done

echo "Script finished."
```

**Key Changes and Why This Works:**

*   **Removed Otter.ai Dependency:**  The
script no longer includes any code related to
Otter.ai.
*   **Blender Command:** The core of the
script now relies entirely on Blender's
command-line interface. The
`"$BLENDER_EXECUTABLE" --background ...`
command is configured to:
    *   `--background`: Runs Blender in the
background, so the terminal is available.
    *   `--python /path/to/your/script.py`:
This line is *crucial*.  It specifies that you
want to execute a Python script to control
Blender.  (I'll show you the script below.)
    *   `--script-binding "$VIDEO_FILE"`: This
tells Blender to bind the audio track of the
video to the script. The script then uses
Blender's API to access the audio and generate
the transcript.
    *   `--output "$OUTPUT_VIDEO_FILE"`: This
specifies the output file.

**The Python Script
(`/path/to/your/script.py`)**

You **must** create a Python script at the
location specified by the `--script-binding`
option in the Blender command. This script
will contain the code to actually access the
audio and generate the transcript.

```python
import bpy
import subprocess

# Get the video file
video_file = bpy.data.filepath

# Create the output file name
output_file = video_file.replace(".mp4",
"_with_subtitles.mp4")

#  This is a placeholder - replace with actual
audio-to-text command.
# This example just creates a dummy video for
demonstration.
# You'll need to implement the audio-to-text
part.

# Create a dummy video (replace with actual
audio processing)
bpy.context.scene.camera.data.type =
'perspective'
bpy.context.scene.camera.data.angle = 60
bpy.context.scene.camera.data.lens = 35

bpy.context.scene.frame_duration = 1.0
bpy.context.scene.frame_end = 100

# Render the video
bpy.context.scene.render.filepath =
output_file
bpy.context.scene.render.resolution_output_filebpy.context.scene.render.resolution_x = 1920
bpy.context.scene.render.resolution_y = 1080
bpy.ops.render.render(write_still=True)

print(f"Output video saved to: {output_file}")
```

**Explanation of the Python Script:**

1.  **Import `bpy`:** The `bpy` module is
Blender's Python API.
2.  **Get Video File Path:**
`bpy.data.filepath` gets the full path to the
video file.
3.  **Create Output File Name:** Constructs
the name of the output file with subtitles.
4.  **Dummy Rendering:** The script currently
just renders a blank video frame. Youâ€™ll
replace this with your actual audio-to-text
implementation.
5.  **`bpy.ops.render.render()`:** This
function performs the video rendering.

**Important Considerations:**

*   **Blender Audio-to-Text Limitations:**
Blender's built-in audio-to-text feature is
experimental and may not be as accurate or
feature-rich as Otter.ai.  Expect to do some
manual editing of the generated transcript.
The accuracy depends heavily on the audio
quality.
*   **Error Handling:** Add error handling to
the Python script to catch potential issues
during the rendering process.
*   **Customization:**  The Python script can
be customized to change the rendering settings
(resolution, frame rate, etc.).

This approach provides a faster and simpler
workflow than using Otter.ai, but it requires
you to implement the audio-to-text
functionality yourself within the Blender
script.

Let me know if you'd like to explore different
audio-to-text techniques or discuss error
handling!

